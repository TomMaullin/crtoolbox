{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e42aa6c",
   "metadata": {},
   "source": [
    "# Cluster Size Distribution (HPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1a255",
   "metadata": {},
   "source": [
    "## Pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask\n",
    "!pip install crtoolbox\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f080e2f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b689fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask imports\n",
    "from dask.distributed import Client, as_completed\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import supporting functions\n",
    "from crtoolbox.lib.boundary import *\n",
    "from crtoolbox.lib.regression import *\n",
    "\n",
    "# Import bootstrap functions\n",
    "from crtoolbox.bootstrap import *\n",
    "\n",
    "# Import data generation\n",
    "from crtoolbox.tests.generate_2d_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9756ed",
   "metadata": {},
   "source": [
    "## Simulation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfdbb2",
   "metadata": {},
   "source": [
    "Bootstrap function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(resids,bdry_weights,sigma_bdry_interp_concat,muhat_grad_bdry_interp_concat,n,n_boot=5000):\n",
    "\n",
    "    # Dimensions of bootstrap variables\n",
    "    boot_dim = np.array([n, 1, 1]) \n",
    "\n",
    "    # Initialize numpy array of zeros to store integral values\n",
    "    integral_values = np.zeros((1,n_boot))\n",
    "\n",
    "    # Perform bootstrap\n",
    "    for b in np.arange(n_boot):\n",
    "\n",
    "        print('Boot: ' + str(b))\n",
    "\n",
    "        # Obtain bootstrap variables\n",
    "        boot_vars = 2*np.random.randint(0,2,boot_dim,dtype=\"int8\")-1\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # Bootstrap residuals\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # Multiply by rademacher variables\n",
    "        boot_resids = boot_vars*resids\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # Get gi along dalpha FcHat\n",
    "        # ------------------------------------------------------\n",
    "        # Sum across subjects to get the bootstrapped a values along\n",
    "        # the boundary of dalphaFcHat. (Note: For some reason this is \n",
    "        # much faster if performed seperately for each of the last rows. \n",
    "        # I am still looking into why this is)\n",
    "        mean_outer = np.sum(boot_resids[...,0], axis=0)/n\n",
    "        mean_inner = np.sum(boot_resids[...,1], axis=0)/n\n",
    "\n",
    "        # Get sum of squares\n",
    "        ssq_outer = np.sum((boot_resids[...,0]-mean_outer)**2, axis=0)/(n-1)\n",
    "        ssq_inner = np.sum((boot_resids[...,1]-mean_inner)**2, axis=0)/(n-1)\n",
    "\n",
    "        # Obtain bootstrap G\n",
    "        boot_G_outer = np.sqrt(n)*mean_outer/np.sqrt(ssq_outer)\n",
    "        boot_G_inner = np.sqrt(n)*mean_inner/np.sqrt(ssq_inner)\n",
    "\n",
    "        # Work out weights\n",
    "        outer_weights = bdry_weights[...,0]\n",
    "        inner_weights = bdry_weights[...,1]\n",
    "\n",
    "\n",
    "        # Work out interpolated values\n",
    "        boot_G_bdry_interp_concat = inner_weights*boot_G_inner + outer_weights*boot_G_outer\n",
    "\n",
    "        # Get bootstrap estimate of integral\n",
    "        integral_value = np.sum(sigma_bdry_interp_concat*boot_G_bdry_interp_concat/muhat_grad_bdry_interp_concat)\n",
    "\n",
    "        # Save integral value\n",
    "        integral_values[0,b] = integral_value\n",
    "\n",
    "    # Sort integral values\n",
    "    integral_values[0,:] = np.sort(integral_values[0,:].reshape(n_boot))\n",
    "\n",
    "    # Return integral values\n",
    "    return integral_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62349b1f",
   "metadata": {},
   "source": [
    "Simulation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will run n_sim simulations and save the results to a csv file.\n",
    "def run_simulation(n_sim, n, sim_instance, out_dir,n_boot=5000):\n",
    "\n",
    "    # Obtain data directory\n",
    "    out_dir = os.path.join(out_dir, str(n) + 'subjects')\n",
    "    \n",
    "    # Obtain data directory\n",
    "    data_dir = os.path.join(out_dir, 'instance_' + str(sim_instance))\n",
    "\n",
    "    # Initialize array of zeros of length n_sim for clt_est\n",
    "    clt_est_store = np.zeros(n_sim)\n",
    "\n",
    "    # Initialize array of zeros of length n_sim for integral values\n",
    "    sim_integral_values = np.zeros(n_sim)\n",
    "\n",
    "    # Initialize array of zeros of size (n_sim,n_boot) for boot integral values\n",
    "    boot_integral_values = np.zeros((n_sim,n_boot))\n",
    "\n",
    "    # Empirical cdf\n",
    "    for i in np.arange(n_sim):\n",
    "\n",
    "        print('Sim: ' + str(i))\n",
    "        \n",
    "        # This command will define a circular smooth signal\n",
    "        signal = CircleSignal(r=250, fwhm=[100,100], mag=3,dim=[1000,1000])\n",
    "\n",
    "        # Generate signal\n",
    "        signal.generate()\n",
    "\n",
    "        # Get signal\n",
    "        mu = signal.mu\n",
    "        \n",
    "        # Specify the noise \n",
    "        noise = Noise(var=1,fwhm=[60,60],n=n,dim=[1000,1000])   \n",
    " \n",
    "        # Threshold c\n",
    "        c = 2\n",
    "\n",
    "        # We can generate some 2D data using the generate_data_2D function\n",
    "        data_files, mu_file = generate_data_2D(signal, noise, out_dir=data_dir)\n",
    "\n",
    "        # Construct a design matrix with only an intercept (this means that the parameter estimate from the regression will be the mean of the data)\n",
    "        X = np.ones((n,1))\n",
    "\n",
    "        # Perform regression to get residuals, muhat, and sigma\n",
    "        muhat_file, sigma_file, resid_files = regression(data_files, X, out_dir=data_dir)\n",
    "\n",
    "        # Read in muhat and sigma\n",
    "        muhat = read_image(muhat_file)\n",
    "        sigma = read_image(sigma_file)\n",
    "\n",
    "        # Compute G = (muhat-mu)/(sigma/sqrt(n))\n",
    "        G = (muhat-mu)/(sigma/np.sqrt(n))\n",
    "\n",
    "        # Get tau\n",
    "        tau = 1/np.sqrt(n) \n",
    "\n",
    "        # Ac \n",
    "        Ac = mu > c\n",
    "\n",
    "        # AcHat\n",
    "        AcHat = muhat > c\n",
    "        \n",
    "        def thicken_set(binary_map):\n",
    "\n",
    "            # Generate boundary map\n",
    "            bdry = get_bdry_map_combined(binary_map)\n",
    "\n",
    "            # Add bdry to binary map and rethreshold\n",
    "            binary_map = np.logical_or(binary_map, bdry)\n",
    "\n",
    "            # Return binary map\n",
    "            return binary_map\n",
    "\n",
    "#         # Thicken sets\n",
    "#         AcHat_thickened = thicken_set(AcHat)\n",
    "#         Ac_thickened = thicken_set(Ac)\n",
    "\n",
    "        # Get size of estimated cluster\n",
    "        est_clus_size = np.sum(AcHat)\n",
    "\n",
    "        # Get size of true cluster\n",
    "        true_clus_size = np.sum(Ac)\n",
    "\n",
    "        # Get estimated clt value\n",
    "        clt_est = (est_clus_size - true_clus_size)/tau\n",
    "\n",
    "#         # Get size of estimated cluster thickened\n",
    "#         est_clus_size_thickened = np.sum(AcHat_thickened)\n",
    "\n",
    "#         # Get size of true cluster thickened\n",
    "#         true_clus_size_thickened = np.sum(Ac_thickened)\n",
    "        \n",
    "#         # Get estimated clt value thickened\n",
    "#         clt_est_thickened = (est_clus_size_thickened - true_clus_size_thickened)/tau\n",
    "        \n",
    "        # Record clt_est\n",
    "        clt_est_store[i] = clt_est #+ clt_est_thickened)/2\n",
    "\n",
    "        # Get boolean maps for the boundary of AcHat\n",
    "        AcHat_bdry_map = get_bdry_maps(muhat, c)\n",
    "\n",
    "        # Get coordinates for the boundary of AcHat\n",
    "        AcHat_bdry_locs = get_bdry_locs(AcHat_bdry_map)\n",
    "\n",
    "        # Obtain the muhat values along the boundary for AcHat\n",
    "        muhat_bdry_vals_concat = get_bdry_values_concat(muhat, AcHat_bdry_locs)\n",
    "\n",
    "        # Obtain the weights along the boundary for AcHat\n",
    "        AcHat_bdry_weights_concat = get_bdry_weights_concat(muhat_bdry_vals_concat, c)\n",
    "\n",
    "        # Get G values along the boundary of AcHat\n",
    "        G_bdry_vals_concat = get_bdry_values_concat(G, AcHat_bdry_locs)\n",
    "\n",
    "        # Interpolate G along the boundary of AcHat\n",
    "        G_bdry_interp_concat = get_bdry_vals_interpolated_concat(G_bdry_vals_concat, AcHat_bdry_weights_concat)\n",
    "\n",
    "        # Obtain the sigmahat values along the boundary for AcHat\n",
    "        sigma_bdry_vals_concat = get_bdry_values_concat(sigma, AcHat_bdry_locs)\n",
    "\n",
    "        # Interpolate sigma along the boundary of AcHat\n",
    "        sigma_bdry_interp_concat = get_bdry_vals_interpolated_concat(sigma_bdry_vals_concat, AcHat_bdry_weights_concat)\n",
    "\n",
    "        # Obtain the gradient of muhat from numpy\n",
    "        muhat_grad = np.gradient(muhat)\n",
    "\n",
    "        # Obtain the magnitude of the gradient from the partial derivatives\n",
    "        muhat_grad = np.sqrt(muhat_grad[0]**2 + muhat_grad[1]**2)\n",
    "\n",
    "        # Get muhat_grad values along the boundary of AcHat\n",
    "        muhat_grad_bdry_vals_concat = get_bdry_values_concat(muhat_grad, AcHat_bdry_locs)\n",
    "\n",
    "        # Interpolate muhat_grad along the boundary of AcHat\n",
    "        muhat_grad_bdry_interp_concat = get_bdry_vals_interpolated_concat(muhat_grad_bdry_vals_concat, AcHat_bdry_weights_concat)\n",
    "\n",
    "        # Integral value\n",
    "        sim_integral_value = np.sum(sigma_bdry_interp_concat*G_bdry_interp_concat/muhat_grad_bdry_interp_concat)\n",
    "\n",
    "        # Save integral value\n",
    "        sim_integral_values[i] = sim_integral_value\n",
    "\n",
    "        # Loop through subjects\n",
    "        for j in np.arange(n):\n",
    "\n",
    "            # Obtain residuals\n",
    "            resid = read_images(resid_files[j])[...,0]\n",
    "\n",
    "            # Standardize residuals\n",
    "            resid = (resid/sigma).reshape((1,) + resid.shape)\n",
    "\n",
    "            # Residuals along boundary for current subject\n",
    "            resid_bdry_concat_j = get_bdry_values_concat(resid, AcHat_bdry_locs)\n",
    "\n",
    "            # Concatenate residuals\n",
    "            if j == 0:\n",
    "                resids = resid_bdry_concat_j\n",
    "            else:\n",
    "                resids = np.concatenate((resids, resid_bdry_concat_j), axis=0)\n",
    "\n",
    "        # Run bootstrap\n",
    "        boot_integral_values_i = bootstrap(resids, AcHat_bdry_weights_concat, sigma_bdry_interp_concat,muhat_grad_bdry_interp_concat, n, n_boot=n_boot)\n",
    "\n",
    "        # Save integral values\n",
    "        boot_integral_values[i,:] = boot_integral_values_i\n",
    "\n",
    "    # Sort clt_est_store\n",
    "    clt_est_store = np.sort(clt_est_store.reshape(n_sim))\n",
    "\n",
    "    # Save clt_est_store\n",
    "    append_to_file(os.path.join(out_dir,'clt_est_store.csv'), clt_est_store)\n",
    "\n",
    "    # Sort sim_integral_values\n",
    "    sim_integral_values = np.sort(sim_integral_values.reshape(n_sim))\n",
    "\n",
    "    # Save sim_integral_values\n",
    "    append_to_file(os.path.join(out_dir,'sim_integral_values.csv'), sim_integral_values)\n",
    "\n",
    "    # Save boot_integral_values\n",
    "    append_to_file(os.path.join(out_dir,'boot_integral_values.csv'), boot_integral_values)\n",
    "\n",
    "    # Remove other files\n",
    "    remove_files([mu_file, *data_files, muhat_file, sigma_file, *resid_files])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73a873",
   "metadata": {},
   "source": [
    "Cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset previous simulation run\n",
    "def reset_sim(n, sim_instance, out_dir):\n",
    "    \n",
    "    # Obtain output directory\n",
    "    out_dir = os.path.join(out_dir, str(n) + 'subjects')\n",
    "    \n",
    "    # Obtain data directory\n",
    "    data_dir = os.path.join(out_dir, 'instance_' + str(sim_instance))\n",
    "\n",
    "    # Remove data dir\n",
    "    if os.path.exists(data_dir):\n",
    "\n",
    "        # Remove data dir\n",
    "        shutil.rmtree(data_dir)\n",
    "    \n",
    "    # If files exist remove them\n",
    "    if os.path.isfile(os.path.join(out_dir,'clt_est_store.csv')) or os.path.isfile(os.path.join(out_dir,'sim_integral_values.csv')) or os.path.isfile(os.path.join(out_dir,'boot_integral_values.csv')):\n",
    "\n",
    "        # Remove files\n",
    "        remove_files([os.path.join(out_dir,'clt_est_store.csv'), os.path.join(out_dir,'sim_integral_values.csv'), os.path.join(out_dir,'boot_integral_values.csv')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f2d80",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bad629",
   "metadata": {},
   "source": [
    "Run a simulation instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d568864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory (feel free to change this to your desired output directory)\n",
    "out_dir = os.path.join(os.getcwd(),'results')\n",
    "\n",
    "# Number of simulations\n",
    "n_sim = 30\n",
    "\n",
    "# Number of subjects\n",
    "n = 100\n",
    "\n",
    "# Number for simulation instance\n",
    "sim_instance = 39\n",
    "\n",
    "# Run a simulation with 100 instance and 100 subjects\n",
    "#run_simulation(n_sim, n, sim_instance, out_dir, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e72281",
   "metadata": {},
   "source": [
    "Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaaf142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain data directory\n",
    "data_dir = os.path.join(out_dir, str(n) + 'subjects')\n",
    "\n",
    "# Read in results\n",
    "clt_est_store = pd.read_csv(os.path.join(data_dir,'clt_est_store.csv'), header=None).values\n",
    "sim_integral_values = pd.read_csv(os.path.join(data_dir,'sim_integral_values.csv'), header=None).values\n",
    "boot_integral_values = pd.read_csv(os.path.join(data_dir,'boot_integral_values.csv'), header=None).values\n",
    "\n",
    "# Plot empirical cdf of clt_est_store\n",
    "plt.plot(np.sort(clt_est_store), np.linspace(0, 1, n_sim, endpoint=False))\n",
    "\n",
    "# Plot empirical cdf of boot_integral_values, averaged over simulations\n",
    "plt.plot(np.mean(boot_integral_values, axis=0), np.linspace(0, 1, len(np.mean(boot_integral_values, axis=0)), endpoint=False))\n",
    "\n",
    "# Plot empirical cdf of sim_integral_values\n",
    "plt.plot(sim_integral_values, np.linspace(0, 1, len(sim_integral_values), endpoint=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd410484",
   "metadata": {},
   "source": [
    "Clean up the test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea491bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_sim(n, sim_instance, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed921a",
   "metadata": {},
   "source": [
    "## Cluster Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26df6f",
   "metadata": {},
   "source": [
    "Simulation variables for running on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for running on cluster\n",
    "n_sim = 500\n",
    "\n",
    "# Number of nodes\n",
    "n_node = 200\n",
    "\n",
    "# Number of sims per node\n",
    "n_sim_per_node = n_sim//n_node\n",
    "\n",
    "# Range of subject values\n",
    "n_subs = np.array([200])#[25,50,100,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e0401",
   "metadata": {},
   "source": [
    "Boot up 100 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SLURM Cluster\n",
    "cluster = SLURMCluster()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Connect to client\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "\n",
    "# Ask for 100 nodes for computation\n",
    "cluster.scale(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty futures list\n",
    "futures = []\n",
    "\n",
    "# Loop through subjects\n",
    "for n in n_subs:\n",
    "\n",
    "    # Submit jobs\n",
    "    for instance in np.arange(n_node):\n",
    "\n",
    "        # Run the jobNum^{th} job.run_simulation(n_sim, n, out_dir, 5000)\n",
    "        future_b = client.submit(run_simulation, n_sim_per_node, n, instance, out_dir, pure=False)\n",
    "\n",
    "        # Append to list \n",
    "        futures.append(future_b)\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "# Wait for results\n",
    "for i in completed:\n",
    "    i.result()\n",
    "\n",
    "# Delete the future objects (NOTE: see above comment in setup section).\n",
    "del i, completed, futures, future_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663556a",
   "metadata": {},
   "source": [
    "Shutdown the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ddfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the client\n",
    "client.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f6252",
   "metadata": {},
   "source": [
    "Reset the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd37327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory (feel free to change this to your desired output directory)\n",
    "out_dir = os.path.join(os.getcwd(),'results')\n",
    "\n",
    "# Set to true to delete simulation outputs\n",
    "reset = False\n",
    "\n",
    "# If we are resetting, delete everything\n",
    "if reset:\n",
    "    \n",
    "    # Loop through sample sizes\n",
    "    for n in n_subs:\n",
    "\n",
    "        # Loop through simulation instances\n",
    "        for instance in np.arange(n_node):\n",
    "\n",
    "            # Reset\n",
    "            reset_sim(n, instance, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35ba71",
   "metadata": {},
   "source": [
    "## View and sort results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5282071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subjects\n",
    "n = 200\n",
    "\n",
    "n_sim=500\n",
    "\n",
    "# Obtain data directory\n",
    "data_dir = os.path.join(out_dir, str(n) + 'subjects')\n",
    "\n",
    "# Read in results\n",
    "clt_est_store = pd.read_csv(os.path.join(data_dir,'clt_est_store.csv'), header=None).values\n",
    "sim_integral_values = pd.read_csv(os.path.join(data_dir,'sim_integral_values.csv'), header=None).values\n",
    "boot_integral_values = pd.read_csv(os.path.join(data_dir,'boot_integral_values.csv'), header=None).values\n",
    "\n",
    "# Plot empirical cdf of clt_est_store\n",
    "plt.plot(np.sort(clt_est_store[0:500].reshape(n_sim)), np.linspace(0, 1, n_sim, endpoint=False),label='Empirical (muhat-mu)/tau')\n",
    "\n",
    "# Plot empirical cdf of boot_integral_values, averaged over simulations\n",
    "plt.plot(np.mean(boot_integral_values[0:500,:], axis=0), np.linspace(0, 1, len(np.mean(boot_integral_values[0:500,:], axis=0)), endpoint=False),label='Bootstrap Integral')\n",
    "\n",
    "# Plot empirical cdf of boot_integral_values, averaged over simulations\n",
    "plt.plot(np.percentile(boot_integral_values[0:500,:], 0.95, axis=0), np.linspace(0, 1, len(np.mean(boot_integral_values[0:500,:], axis=0)), endpoint=False))\n",
    "\n",
    "# Plot empirical cdf of boot_integral_values, averaged over simulations\n",
    "plt.plot(np.percentile(boot_integral_values[0:500,:], 0.05, axis=0), np.linspace(0, 1, len(np.mean(boot_integral_values[0:500,:], axis=0)), endpoint=False))\n",
    "\n",
    "# Plot empirical cdf of sim_integral_values\n",
    "plt.plot(np.sort(sim_integral_values[0:500].reshape(n_sim)), np.linspace(0, 1, n_sim, endpoint=False),label='Empirical Integral')\n",
    "\n",
    "plt.title('Simulation results: ' + str(n) + ' subjects')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f6a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
